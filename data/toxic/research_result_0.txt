### ArXiv Papers
1. **DetoxLLM: A Framework for Detoxification with Explanations**  
   The paper presents DetoxLLM, a comprehensive framework for detoxifying toxic language across various platforms while addressing limitations in previous detoxification efforts. It highlights the challenges of existing models, including their focus on specific platforms, lack of explanations for detoxification decisions, and failure to handle non-detoxifiable casesâ€”where toxic text cannot be altered without changing its meaning. DetoxLLM introduces a cross-platform pseudo-parallel corpus generated through advanced data processing techniques using ChatGPT, enabling the training of detoxification models that outperform state-of-the-art (SoTA) models trained on human-annotated data. The framework also incorporates a paraphrase detector to identify non-detoxifiable cases and provides explanations for its detoxification decisions, enhancing transparency and user trust. Through extensive experiments, DetoxLLM demonstrates superior performance in detoxification tasks, robustness against adversarial toxicity, and effective handling of implicit hate speech. The paper concludes with a discussion of the framework's contributions, limitations, and ethical considerations, emphasizing its potential for improving online communication by reducing toxic language.

2. **Can LLMs Recognize Toxicity? A Structured Investigation Framework and Toxicity Metric**  
   This paper presents a new metric, LATTE (LLMs As Toxicity Evaluator), aimed at assessing the toxicity of text generated by Large Language Models (LLMs). The authors argue that existing toxicity metrics, which rely on encoder models trained on specific datasets, are limited by their susceptibility to out-of-distribution problems and their dependence on the definitions of toxicity provided by those datasets. The study introduces a structured investigation framework that analyzes toxicity factors and evaluates the intrinsic toxic attributes of LLMs to determine their effectiveness as evaluators. The authors identify three key factors of toxicity: demeaning content, partiality, and ethical preference. They demonstrate that their proposed metric, LATTE, significantly outperforms traditional metrics, achieving improvements of over 12 points in F1 score. The research highlights the importance of context in defining toxicity and emphasizes the need for flexible metrics that can adapt to varying definitions of toxicity. The authors also find that LLMs exhibit biases that can affect their performance in toxicity evaluations, particularly in unverified contexts. The paper concludes that while LLMs can be effective in measuring toxicity within certain domains, caution is required when applying them to broader contexts due to their inherent biases and limitations.

3. **Toxicity Detection in Drug Candidates using Simplified Molecular Descriptors**  
   This article discusses the use of artificial intelligence (AI) tools, specifically Long Short-Term Memory (LSTM) models, to detect toxicity in drug candidates using the Simplified Molecular Input Line Entry System (SMILES) as a parameter. It highlights the importance of toxicity analysis in drug development, noting that toxicity is a major reason for the rejection of drug candidates, contributing to high development costs. The authors explore various neural network architectures, including Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN), emphasizing LSTM's effectiveness in handling sequential data like SMILES for toxicity prediction. The methodology involves training the LSTM model on publicly available datasets, addressing issues of data imbalance through undersampling. Results indicate promising accuracy and potential for real-world applications, suggesting that AI models can significantly enhance the efficiency of toxicity detection in drug development.

### Github Links
1. [RafayKhattak/ToxiScan: ToxiScan is an advanced text analysis tool that detects toxicity in textual data using Natural Language Toolkit (NLTK), TfidfVectorizer, and a Naive Bayes classifier.](https://github.com/RafayKhattak/ToxiScan)  
   **Summary**: ToxiScan features a user-friendly interface built with Streamlit, allowing easy access to toxicity analysis. It includes comprehensive text preprocessing and feature extraction for accurate predictions. The model is trained on the "Toxic Tweets Dataset" from Kaggle, ensuring effective recognition of toxic patterns. Users can install ToxiScan locally, input text for analysis, and receive toxicity predictions. The repository provides clear instructions for setup and usage, making it accessible for developers and researchers interested in toxicity detection.

2. [unitaryai/detoxify: Detoxify is a library designed for toxic comment classification using Pytorch Lightning and Hugging Face Transformers.](https://github.com/unitaryai/detoxify)  
   **Summary**: Detoxify includes models trained on three Jigsaw challenges: Toxic Comment Classification, Unintended Bias in Toxicity Classification, and Multilingual Toxic Comment Classification. It features improved multilingual and unbiased models, achieving high AUC scores and consistent class naming conventions. The library supports multiple languages and provides easy-to-use prediction functions, intended for research and content moderation. The repository contains comprehensive documentation, examples, and pre-trained models, making it a valuable resource for developers looking to implement toxicity detection in their applications.

3. [Creating a Github action to detect toxic comments using TensorFlow](https://charliegerard.dev/blog/github-action-toxic-comments/)  
   **Summary**: This blog post describes the creation of a GitHub action that detects toxic comments using TensorFlow. It outlines the steps taken to implement the action, including setting up the environment, training the model, and integrating it into the GitHub workflow for automated toxicity detection. The post provides code snippets and explanations, making it a practical guide for developers interested in automating toxicity detection in their projects.

### Hugging Face Models
1. [PolyGuard](https://huggingface.co/Jayveersinh-Raj/PolyGuard)  
   **Summary**: The README outlines a cross-lingual zero-shot transfer model designed for detecting and flagging toxic or abusive comments across multiple languages. It excels in identifying severe toxicity, achieving a 100% accuracy rate in classifying non-toxic sentences. The model utilizes datasets from Google Jigsaw and Wikipedia, employing a zero-shot mechanism for multilingual support. The repository includes usage instructions, model details, and performance metrics, making it suitable for developers seeking to implement multilingual toxicity detection.

2. [EthicalEye](https://huggingface.co/autopilot-ai/EthicalEye)  
   **Summary**: Ethical Eye is an open-source AI model developed to analyze and flag user-generated content for harmful or unethical behavior. It employs text classification and toxicity analysis using the XLM-Roberta architecture, supporting multiple languages. The model aims to foster safer online environments while acknowledging the need for human oversight in its application. The repository provides detailed documentation, including training procedures and evaluation metrics, making it a useful resource for researchers and developers focused on ethical AI applications.

3. [toxic-bert-model](https://huggingface.co/Dolcevitta/toxic-bert-model)  
   **Summary**: This BERT-based model is designed for detecting toxic comments in English text. It performs binary classification, categorizing comments as either toxic or non-toxic, achieving a validation accuracy of 79.50%. The model is intended for applications such as automatic comment moderation and toxicity analysis in digital conversations. The repository includes model training details, usage examples, and performance benchmarks, providing a comprehensive resource for developers interested in integrating toxicity detection into their systems.

### Hugging Face Datasets
1. [NMToxification](https://huggingface.co/datasets/ovakimyanchris/NMToxification)  
   **Summary**: This dataset focuses on toxicity analysis in text, consisting of parallel corpora where toxic prompts have been translated to non-toxic versions. It is publicly accessible and tagged for tasks related to text generation, primarily in English. The dataset is structured to facilitate training and evaluation of detoxification models, making it a valuable resource for researchers in the field.

2. [Aegis-AI-Content-Safety-Dataset-1.0](https://huggingface.co/datasets/nvidia/Aegis-AI-Content-Safety-Dataset-1.0)  
   **Summary**: This open-source dataset consists of approximately 11,000 manually annotated interactions categorized under critical risk areas, including hate speech and violence. It aims to help researchers develop content moderation guardrails for large language models. The dataset is designed to support the training of models that can effectively identify and mitigate harmful content in user-generated text.

3. [multilingual_toxicity_dataset](https://huggingface.co/datasets/textdetox/multilingual_toxicity_dataset)  
   **Summary**: This dataset includes approximately 45,000 rows of text data in multiple languages, comprising 5,000 samples per language, split evenly between toxic and non-toxic comments. It is designed for binary toxicity classification and is structured in Parquet format. The dataset is useful for training and evaluating multilingual toxicity detection models, providing a diverse set of examples for robust model performance.

This comprehensive summary includes detailed insights from the arXiv papers, GitHub repositories, Hugging Face models, and datasets related to developing a service that analyzes text input for toxicity levels. Each section provides valuable information for researchers and developers working in this domain.